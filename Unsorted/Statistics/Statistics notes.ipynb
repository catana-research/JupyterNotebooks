{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics\n",
    "\n",
    "<i>Notes from https://arxiv.org/pdf/1503.07622.pdf</i>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions\n",
    "\n",
    "Roman letters ($x$, ...) - Observables <br>\n",
    "Greek letters ($\\mu$, ...) - Model parameters <br>\n",
    "Dataset - $D = \\{x_{1}, . . . , x_{n}\\}$ <br>\n",
    "$e$ - Event <br>\n",
    "$c$ - Channel <br>\n",
    "$\\hat{\\alpha}$ - Estimator for $\\alpha$  <br>\n",
    "Test statistic - A function that maps the data to a single real number: $T(D) \\rightarrow {\\rm I\\!R}$  <br>\n",
    "\n",
    "\n",
    "Type-I error, $α$ - The probability the null hypothesis will be rejected when it is true. This is equivalent to the probability under the null hypothesis that the data will not be found in this acceptance region, ie. $α = P(T(D) ≥ k_{α}|H_{0})$. <br>\n",
    "Type-II error, $β$ - The probability the null hypothesis will be accepted when the alternate is true. The probability is given by $β = P(T(D) < k_{α}|H_{1})$. <br>\n",
    "Power - $1 − β$ <br>\n",
    "$α_{poi}$ - Parameters of interest. <br>\n",
    "$α_{nuis}$ - Nuisance parameters. <br>\n",
    "Coverage - The probability that the interval will contain (cover) the parameter $α$ when it is true, ${\\rm coverage}(α) = P(α ∈ I | α)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Poisson model\n",
    "\n",
    "If we consider the\n",
    "events as independently drawn from the same underlying distribution, then clearly the probability density\n",
    "is just a product of densities for each event. However, if we have a prediction that the total number of\n",
    "events expected, call it $\\nu$, then we should also include the overall Poisson probability for observing $n$\n",
    "events given $\\nu$ expected. Thus, we arrive at what statisticians call a marked Poisson model,\n",
    "\n",
    "\n",
    "$$f(D|\\nu, \\alpha) = Pois(n|\\nu)\n",
    "\\prod_{e=1}^{n}\n",
    "f(x_{e}|\\alpha)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEKCAYAAADjDHn2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x18df779e278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def poisson(k, mu):\n",
    "    return math.exp(-mu) * mu**(k)/math.factorial(k)\n",
    "    \n",
    "xMin = 0\n",
    "xMax = 10\n",
    "N    = xMax - xMin + 1\n",
    "mu   = 3\n",
    "\n",
    "x = np.linspace(xMin, xMax, N)\n",
    "y = [poisson(x_i, mu) for x_i in x]\n",
    "plt.plot(x,y,'bo')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('Probability')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Likelihood function\n",
    "\n",
    "The likelihood function $L(\\alpha)$ is numerically equivalent to $f(x|\\alpha)$ with $x$ fixed – or $f(D|α)$ with\n",
    "$D$ fixed. <br>\n",
    "\n",
    "$$−\\ln L(\\alpha) = \\{\\nu(\\alpha) − n \\ln \\nu(\\alpha)\\}\n",
    "− \\sum_{e=1}^{n} \\ln f(x_{e}) + \\{\\ln n!\\}$$\n",
    "\n",
    "First term is the 'extended term', last term is a constant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayes' theorem\n",
    "$$P(A|B) = \\frac{P(B|A)P(A)}{P(B)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To interpret an example measurement of $n_{CR}$ as implying a probability distribution for $\\nu_{B}$\n",
    "we would write:\n",
    "$$\\pi(\\nu_B{}|n_{CR}) \\propto f(n_{CR}|\\nu_{B})\\eta(\\nu_{B})$$\n",
    "Where $\\pi(\\nu_B{}|n_{CR})$ is the <it>posterior</it> probability density, $f(n_{CR}|\\nu_{B})$ is the likelihood function, and $\\eta(\\nu_{B})$ is the <it>prior</it> probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measurement as parameter estimation\n",
    "\n",
    "The sample mean $\\overline{x} = \\sum_{e=1}^{n} x_{e}/n$ is an estimate for\n",
    "the mean, $\\mu$, of a Gaussian probability density $f(x|\\mu, \\sigma) = Gauss(x|\\mu, \\sigma)$. More generally, an estimator\n",
    "$\\hat{\\alpha}(D)$ is some function of the data and its value is used to estimate the true value of some parameter $\\alpha$. <br>\n",
    "\n",
    "The bias of an estimator is defined as $B(\\hat{\\alpha}) = E[\\hat{\\alpha}] − \\alpha$, where $E$ means the expectation value of $E[\\hat{\\alpha}] = \\int\\hat{α}(x)f(x)dx$ or the probability-weighted average. \n",
    "\n",
    " The variance of an estimator is defined as: $var[\\hat{\\alpha}] = E[(\\alpha − E[\\hat{\\alpha}])^{2}]$. Clearly one would like\n",
    "an estimator with the minimum variance. \n",
    "\n",
    "Unfortunately, there is a tradeoff between bias and variance. Physicists tend to be allergic to biased estimators, and within the class of unbiased estimators, there is a well defined minimum variance bound referred to as the Cramér-Rao bound."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum likelihood estimator\n",
    "\n",
    "The most widely used estimator in physics is the maximum likelihood estimator (MLE). It is\n",
    "defined as the value of $\\alpha$ which maximizes the likelihood function $L(\\alpha)$. Equivalently this value, $\\hat{\\alpha}$,\n",
    "maximizes $\\log L(\\alpha)$ and minimizes $− \\log L(\\alpha)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When one has a multi-parameter likelihood function $L(α)$, then the situation is slightly more\n",
    "complicated. The maximum likelihood estimate for the full parameter list, $\\hat{α}$, is clearly defined. The\n",
    "various components $\\hat{α}_{p}$ are referred to as the unconditional maximum likelihood estimates. In the physics\n",
    "jargon, one says all the parameters are ‘floating’. \n",
    "\n",
    "One can also ask about maximum likelihood estimate\n",
    "of $α_{p}$ is with some other parameters $α_{o}$ fixed; this is called the conditional maximum likelihood estimate\n",
    "and is denoted $\\hat{\\hat{α}}_{p}(α_{o})$. These are important quantities for defining the profile likelihood ratio.\n",
    "\n",
    "The concept of variance of the estimates is also generalized to the covariance matrix $cov[α_{p}, α_{p'}] = E[(\\hat{α}_{p} − α_{p})(\\hat{α}_{p'} − α_{p'})]$ and is often denoted Σ_{pp'}. Note, the diagonal elements of the covariance matrix are the same as the variance for the individual parameters, ie.\n",
    "$cov[α_{p}, α_{p}] = var[α_{p}]$.\n",
    "\n",
    "In the case of a Poisson model $Pois(n|\\nu)$ the maximum likelihood estimate of $\\nu$ is simply $\\hat{\\nu} = n$.\n",
    "Thus, it follows that the variance of the estimator is $var[\\hat{\\nu}] = var[n] = \\nu$.\n",
    "\n",
    "When the number of events is large, the distribution of maximum likelihood estimates approaches a Gaussian or normal distribution. For small samples this isn’t the case, but this limiting distribution is often referred to as an asymptotic distribution. Furthermore, under most circumstances in particle physics, the maximum likelihood estimate\n",
    "approaches the minimum variance or Cramér-Rao bound. In particular, the inverse of the covariance matrix for the estimates is asymptotically given by:\n",
    "\n",
    "$$Σ^{−1}_{pp'}(α) = E\\left[ \\frac{∂^{2} \\log f(x|α)}{∂α_{p}∂_{p'}}| α \\right]$$, \n",
    "\n",
    "where I have written explicitly that the expectation, and thus the covariance matrix itself, depend on the\n",
    "true value α. The right side is called the (expected) Fisher information matrix. The expectation involves an integral over the observables. Since that integral is difficult to perform\n",
    "in general, one often uses the observed Fisher information matrix to approximate the variance of the\n",
    "estimator by simply taking the matrix of second derivatives based on the observed data\n",
    "\n",
    "\n",
    "$$\\tilde{Σ}^{−1}_{pp'}(α) = \\frac{∂^{2} \\log L(α)}{∂α_{p}∂_{p'}}$$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Hypothesis testing\n",
    "\n",
    "Discovery is formulated in terms of a hypothesis test where the background-only hypothesis plays the role of the null hypothesis and the signal-plusbackground hypothesis plays the roll of the alternative. Roughly speaking, the claim of discovery is a\n",
    "statement that the data are incompatible with the background-only hypothesis.\n",
    "\n",
    "Consider the simplest scenario where one is counting events in the signal region, nSR and expects $\\nu_{B}$ events from background and $\\nu_{S}$ events from the putative signal. Then we have the following hypotheses:\n",
    "\n",
    "\n",
    "| symbol  | statistical name      | physics name          | probability model                |\n",
    "|---------|-----------------------|-----------------------|----------------------------------|\n",
    "| $H_{0}$ | null hypothesis       |background-only        | $Pois(n_{SR}|\\nu_{B})$           |\n",
    "| $H_{1}$ | alternate hypothesis  |signal-plus-background | $Pois(n_{SR}|\\nu_{S} + \\nu_{B})$ |\n",
    "\n",
    "\n",
    "In this simple example it’s fairly obvious that evidence for a signal shows up as an excess of events and\n",
    "a reasonable way to quantify the compatibility of the observed data $n^{0}_{CR}$ and the null hypothesis is to\n",
    "calculate the probability that the background-only would produce at least this many events; the $p$-value\n",
    "\n",
    "$$p =\n",
    "\\sum^{∞}_{n=n^{0}_{SR}} Pois(n| \\nu_{B})$$\n",
    "\n",
    "If this $p$-value is very small, then one might choose to reject the null hypothesis. Note, the $p$-value is not to be interpreted as the probability of the null hypothesis given the data – that is a manifestly Bayesian statement. Instead, the $p$-value is a statement about the probability to have obtained data with a certain property assuming the null hypothesis.\n",
    "\n",
    "A function that maps the data to a single real number is called a test statistic: $T(D) \\rightarrow {\\rm I\\!R}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neyman and Pearson\n",
    "\n",
    "Neyman and Pearson provided a framework for hypothesis testing that addresses the choice of\n",
    "the test statistic. This setup treats the null and the alternate hypotheses in an asymmetric way. \n",
    "\n",
    "First, one defines an acceptance region in terms of a test statistic, such that if $T(D) < k_{α}$ one accepts the\n",
    "null hypothesis. One can think of the $T(D) = k_{α}$ as defining a contour in the space of the data, which\n",
    "is the boundary of this acceptance region. \n",
    "\n",
    "Next, one defines the size of the test, $α$, as the probability the null hypothesis will be rejected when it is true \n",
    "(a so-called Type-I error). This is equivalent to the probability under the null hypothesis that the data will not be \n",
    "found in this acceptance region, ie.\n",
    "$α = P(T(D) ≥ k_{α}|H_{0})$. Note, it is now clear why there is a subscript on $k_{α}$, since the contour level is\n",
    "related to the size of the test. \n",
    "\n",
    "In contrast, if one accepts the null hypothesis when the alternate is true,\n",
    "it is called a Type-II error. The probability to commit a Type-II error is denoted as $β$ and it is given by\n",
    "$β = P(T(D) < k_{α}|H_{1})$. One calls $1 − β$ the power of the test. \n",
    "\n",
    "With these definitions in place, one looks for a test statistic that maximizes the power of the test for a fixed test size.\n",
    "In the case of two simple hypotheses (probability models without any parameters), the test statistic leading to the most powerful test is given by the likelihood ratio: \n",
    "$$T_{NP}(D) = \\frac{f(D|H_{1})}{f(D|H_{0})}$$\n",
    "this result is referred to as the Neyman-Pearson lemma.\n",
    "\n",
    "Unfortunately, there is no equivalent to the Neyman-Pearson lemma for models with several free parameters – so called, composite\n",
    "models. Nevertheless, there is a natural generalization based on the profile likelihood ratio.\n",
    "\n",
    "The test statistic $T$ is a real-valued function of the data, then any particular\n",
    "probability model $f_{tot}(D|α)$ implies a distribution for the test statistic $f(T|α)$. Note, the distribution for\n",
    "the test statistic depends on the value of $α$.\n",
    "\n",
    "Once one has the distribution, then one can calculate the p-value is given by\n",
    "$$p(α) = \\int^{∞}_{T_{0}} f(T|α)dT = \\int f(D|α) θ(T(D) − T_{0}) dD = P(T ≥ T_{0}|\\alpha)$$\n",
    "\n",
    "where $T_{0}$ is the value of the test statistic based on the observed data and $θ$ is the Heaviside function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is natural to say that one should not reject the null hypothesis if the $p$-value is larger than the size of the test for any value of the nuisance parameters. Thus, in a frequentist approach one should either present $p$-value explicitly as a function of $α_{nuis}$ or take its maximal (or supremum) value:\n",
    "\n",
    "$$p_{sup}(α_{poi}) = \\underset{α_{nuis}}{\\rm sup} \\space  p(α_{nuis})$$\n",
    "\n",
    "\n",
    "In most sciences conventional choices of the size of test are 10%, 5%, or 1%. In particle physics, our conventional threshold for discovery is the infamous 5σ criterion – which is a conventional way to refer to $α = 2.87 \\times 10^{-7}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excluded and allowed regions as confidence intervals\n",
    "\n",
    "Often we consider a new physics model that is parametrized by theoretical parameters, to which we ask what values of these theoretical parameters are allowed or excluded given available data.\n",
    "\n",
    "In a frequentist setting, these allowed regions are called confidence intervals or confidence regions,\n",
    "and the parameter points outside them are considered excluded. These are associated with a confidence interval\n",
    "is a confidence level, i.e. the 95% and 68% confidence level. If we repeat the\n",
    "experiments and obtain different data, then these confidence intervals will change. It is useful to think of\n",
    "the confidence intervals as being random in the same way the data are random. The defining property of\n",
    "a 95% confidence interval is that it covers the true value 95% of the time.\n",
    "\n",
    "The procedure for building confidence intervals is called the Neyman Construction, and it is based on ‘inverting’ a\n",
    "series of hypothesis tests.  In particular, for each value of $α$ in the parameter\n",
    "space, one performs a hypothesis test based on some test statistic where the null hypothesis is $α$. \n",
    "\n",
    "Note, that in this context, the null hypothesis is changing for each test and generally is not the background only.\n",
    "If one wants a 95% confidence interval, then one constructs a series of hypothesis test with a size\n",
    "of 5%. The confidence interval $I(D)$ is constructed by taking the set of parameter points where the null\n",
    "hypothesis is accepted.\n",
    "$$I(D) = {α| P(T(D) > k_{α} | α) < α} $$\n",
    "where the final $α$ and the subscript $k_{α}$ refer to the size of the test. Since a hypothesis test with a size\n",
    "of 5% should accept the null hypothesis 95% of the time if it is true, confidence intervals constructed in\n",
    "this way satisfy the defining property.\n",
    "\n",
    "This same property is usually formulated in terms of coverage. Coverage is the probability that the interval will contain (cover) the parameter $α$ when it is true, \n",
    "\n",
    "$${\\rm coverage}(α) = P(α ∈ I | α)$$\n",
    "\n",
    "Intervals produced using the Neyman Construction procedure are said to “cover by construction”; however, one can\n",
    "consider alternative procedures that may either under-cover or over-cover. Undercoverage means that\n",
    "$P(α ∈ I | α)$ is smaller than desired and over-coverage means that $P(α ∈ I | α)$ is larger than desired.\n",
    "Note that in general coverage depends on the assumed true value $α$.\n",
    "\n",
    "\n",
    "Since one typically is only interested in forming confidence intervals on the parameters of interest,\n",
    "then one could use the supremum $p$-value. This procedure ensures that the coverage is at least\n",
    "the desired level, though for some values of α it may over-cover (perhaps significantly). This procedure,\n",
    "which I call the ‘full construction’, is also computationally very intensive when $α$ has many parameters\n",
    "as it require performing many hypothesis tests. In the naive approach where each αp is scanned in a\n",
    "regular grid, the number of parameter points tested grows exponentially in the number of parameters.\n",
    "\n",
    "There is an alternative approach, which I call the ‘profile construction’ and which statisticians call\n",
    "an ‘hybrid resampling technique’ that is approximate to the full construction, but typically has\n",
    "good coverage properties. Regions are the ones that satisfy $T(D) < k_{α}$. Once one has the\n",
    "confidence belt, then one can immediately find the confidence interval for a particular measurement of $x$\n",
    "simply by taking drawing a vertical line for the measured value of $x$ and finding the intersection with the\n",
    "confidence belt. For more complicated models with many parameters the confidence belt will have one axis for the test statistic and one axis for each model parameter.\n",
    "\n",
    "Note, a 95% confidence interval does not mean that there is a 95% chance that the true value of the\n",
    "parameter is inside the interval – that is a manifestly Bayesian statement. One can produce a Bayesian\n",
    "credible interval with that interpretation; however, that requires a prior probability distribution over the\n",
    "parameters. Similarly, for any fixed interval I one can compute the Bayesian credibility of the interval:\n",
    "\n",
    "$$P(α ∈ I | D) = \\frac{\\int_{I}f(D|α)\\pi(α)dα}{\\int f(D|α)\\pi(α)dα} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Student's t-distribution\n",
    "\n",
    "tudent's t-distribution is any member of a family of continuous probability distributions that arises when estimating the mean of a normally distributed population in situations where the sample size is small and population standard deviation is unknown.\n",
    "\n",
    "\n",
    "### Student's t-test\n",
    "\n",
    "The t-test is any statistical hypothesis test in which the test statistic follows a Student's t-distribution under the null hypothesis.\n",
    "\n",
    "Student's t-test is used for assessing the statistical significance of the difference between two sample means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review of Important Concepts\n",
    "https://onlinecourses.science.psu.edu/stat504/node/30/\n",
    "\n",
    "Let $X_{1}, X_{2}, ..., X_{n}$ be a simple random sample from a probability distribution $f(x; θ)$.\n",
    "- A parameter $θ$ of $f(x; θ)$ is a variable that is a characteristic of $f(x; θ)$.\n",
    "- A statistic $T$ is any quantity that can be calculated from a sample; it's a function of $X_{1}, X_{2}, ..., X_{n}$ .\n",
    "- An estimate $\\hat{θ}$ for $θ$ is a single number that is a reasonable value for $θ$.\n",
    "- An estimator $\\hat{θ}$ for θ is a statistic that gives the formula for computing the estimate $\\hat{θ}$.\n",
    "\n",
    "The likelihood of the sample is the joint PDF (or PMF)\n",
    "\n",
    "$L(θ)=f(x_{1},…,x_{n};θ)=\\prod^{n}_{i=1} f(x_{i};θ)$\n",
    "\n",
    "The maximum likelihood estimate (MLE) $\\hat{θ}_{MLE}$ maximizes $L(θ)$.\n",
    "\n",
    "\n",
    "\n",
    "Usually, MLE:\n",
    "\n",
    "- is unbiased, E($\\hat{θ}$) = $θ$\n",
    "- is consistent, $\\hat{θ}$ → $θ$, as n → ∞\n",
    "- is efficient, has small SE($\\hat{θ}$) as n → ∞\n",
    "- is asymptotically normal, ($\\hat{θ}$ − $θ$)SE($\\hat{θ}$) ∼ N(0,1)\n",
    "\n",
    "<b>Confidence intervals</b> measure uncertainty of an estimate. \n",
    "\n",
    "The asymptotic normality of the ML estimate, which gives the approximation:\n",
    "$\\hat{θ} ∼ N(θ, \\hat{V}(θ))$\n",
    "\n",
    "\n",
    "Observed Information: The quantity $−l′′(\\hat{θ};x)$, where $l''$ denotes the second derivative with respect to $θ$, is called the “observed information,” and \n",
    "$1/\\sqrt{−l′′(\\hat{θ};x)}$ is an approximate standard error for $\\hat{θ}$.\n",
    " \n",
    "Fisher information: $I(θ) = −E[l′′(\\hat{θ};x)]$\n",
    "\n",
    "Approximate 95% interval: $\\hat{θ} ± 1.96\\sqrt{\\hat{V}(\\hat{θ})} = \\hat{θ} ± 1.96\\frac{1}{\\sqrt{−l′′(\\hat{θ};x)}} = \\hat{θ} ± 1.96\\frac{1}{\\sqrt{I(\\hat{θ})}}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis testing\n",
    "\n",
    "Three main tests: **Wald**, **Score** and **Likelihood ratio** tests\n",
    "\n",
    "### p-value\n",
    "\n",
    "The p-value is defined as the probability, under the null hypothesis, of obtaining a result equal to or more extreme than what was actually observed. \n",
    "\n",
    "### Wald test\n",
    "\n",
    "$H_{0}: 0 = 0_{0}$\n",
    "\n",
    "Reject $H_{0}$ with two-sided 95% C.I. if:\n",
    "$$\\frac{|\\hat{θ} - θ_{0}|}{\\sqrt{\\hat{V}(\\hat{θ})}} \\geq 1.96$$\n",
    "\n",
    "\n",
    "### Likelihood ratio\n",
    "\n",
    "$H_{0}: 0 = 0_{0}$\n",
    "\n",
    "$$2[l(\\hat{θ}) - l(θ_{0})]$$\n",
    "\n",
    "### Example: Binomial\n",
    "\n",
    "Estimated variance: $$\\hat{V}(\\hat{\\pi}) = \\frac{\\hat{\\pi}(1 - \\hat{\\pi})}{n}$$\n",
    "\n",
    "Leads to the Wald statistic:\n",
    "\n",
    "$$Z^{2} = \\left[  \\frac{\\hat{\\pi} - \\pi_{0}}{ \n",
    "\\sqrt{ \\frac{\\hat{\\pi}(1 - \\hat{\\pi})}{n}}\n",
    "} \\right]^{2}$$\n",
    "\n",
    "Inverting the Wald statistic leads to the interval for $\\pi$:\n",
    "\n",
    "$$\\hat{\\pi} \\pm z_{\\alpha/2}\\sqrt{\\frac{\\hat{\\pi}(1 - \\hat{\\pi})}{n}}$$\n",
    "\n",
    "Alternatively, the log-likelihood is: \n",
    "\n",
    "$$l(\\pi) = y\\log \\pi + (n - y) \\log\\left(\\frac{1 - \\hat{\\pi}}{1 - \\pi_{0}} \\right)$$\n",
    "\n",
    "The likelihood-ratio is thus:\n",
    "\n",
    "$$LR = 2\\left[ y\\log\\left( \\frac{\\hat{\\pi}}{\\pi_{0}} \\right) + (n - y) \\log\\left( \\frac{1 - \\hat{\\pi}}{1 - \\pi_{0}} \\right) \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choice of test statistic\n",
    "\n",
    "- If you believe your (within group) raw data are not normally distributed, use the Mann-Whitney _U-test_.\n",
    "- If you believe you data are normally distributed, but you don't know the SD a-priori, use the _t-test_.\n",
    "- If you believe your data are normally distributed and you know the SD a-priori, use the _z-test_.\n",
    "- If your data are comprised of TWO variables, and BOTH variables are \"categories\" or \"groups\", then you should initially have a green light for Chi-Square. For binary choices.\n",
    "\n",
    "### _t-test_\n",
    "\n",
    "A t-test tests a null hypothesis about two means; most often, it tests the hypothesis that two means are equal, or that the difference between them is zero. The t-test allows you to say either \"we can reject the null hypothesis of equal means at the 0.05 level\" or \"we have insufficient evidence to reject the null of equal means at the 0.05 level.\" \n",
    "\n",
    "The t-test is based on Student’s t-distribution. \n",
    "\n",
    "$$t = \\frac{Z}{s} = \\left(\\bar{X} - \\mu\\right) / \\left( \\frac{s}{\\sqrt{n}} \\right)$$\n",
    "\n",
    "Where $s$ is the sample standard deviation: \n",
    "$$s = \\sqrt{\\frac{1}{N - 1} \\sum_{i=1}^{N}(x_{i} - \\bar{x})^{2}}$$.\n",
    "\n",
    "### _z-test_\n",
    "\n",
    "A z-test is a statistical test used to determine whether two population means are different when the variances are known and the sample size is large. The test statistic is assumed to have a normal distribution, and nuisance parameters such as standard deviation should be known in order for an accurate z-test to be performed.\n",
    "\n",
    "z-test relies on the assumption that the distribution of sample means is normal. \n",
    "\n",
    "$$z = \\left(\\bar{X} - \\mu\\right) / \\left( \\frac{\\sigma}{\\sqrt{n}} \\right)$$\n",
    "\n",
    "Where $\\sigma$ is the sample standard deviation.\n",
    "\n",
    "\n",
    "### _$\\chi^{2}$-test_\n",
    "\n",
    "A chi-square test tests a null hypothesis about the relationship between two variables. For example, you could test the hypothesis that men and women are equally likely to vote \"Democratic,\" \"Republican,\" \"Other\" or \"not at all.\" A chi-square test allows you to say either \"we can reject the null hypothesis of no relationship at the 0.05 level\" or \"we have insufficient evidence to reject the null at the 0.05 level.\" \n",
    "\n",
    "A statistic tested by a $\\chi^{2}$ test has $\\chi^{2}$ distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Poisson conditional probability\n",
    "\n",
    "If $$X \\sim Bin(n, p)$$ and $$Y \\sim Bin(X, p)$$\n",
    "\n",
    "I.e. the $n$ parameter of Y depends on output of X, then:\n",
    "\n",
    "$$Y \\sim Bin(n, p \\cdot q)$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
