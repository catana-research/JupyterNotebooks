{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning book"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most algorithms used for deep learning fall somewhere in between, using more than one but less than all of the training examples. These were traditionally called\n",
    "minibatch or minibatch stochastic methods and it is now common to simply\n",
    "call them stochastic methods.\n",
    "The canonical example of a stochastic method is stochastic gradient descent,\n",
    "presented in detail in section 8.3.1.\n",
    "\n",
    "Minibatch sizes are generally driven by the following factors:\n",
    "\n",
    "• Larger batches provide a more accurate estimate of the gradient, but with less than linear returns.\n",
    "\n",
    "• Multicore architectures are usually underutilized by extremely small batches.\n",
    "This motivates using some absolute minimum batch size, below which there is no reduction in the time to process a minibatch.\n",
    "\n",
    "• If all examples in the batch are to be processed in parallel (as is typically the case), then the amount of memory scales with the batch size. For many hardware setups this is the limiting factor in batch size.\n",
    "\n",
    "• Some kinds of hardware achieve better runtime with specific sizes of arrays. Especially when using GPUs, it is common for power of 2 batch sizes to offer better runtime. Typical power of 2 batch sizes range from 32 to 256, with 16 sometimes being attempted for large models.\n",
    "\n",
    "• Small batches can offer a regularizing effect (Wilson and Martinez, 2003), perhaps due to the noise they add to the learning process. Generalization error is often best for a batch size of 1. Training with such a small batch\n",
    "size might require a small learning rate to maintain stability due to the high variance in the estimate of the gradient. The total runtime can be very high due to the need to make more steps, both because of the reduced learning rate and because it takes more steps to observe the entire training set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
