{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# neuralnetworksanddeeplearning\n",
    "\n",
    "http://neuralnetworksanddeeplearning.com/chap1.html\n",
    "\n",
    "Perceptrons were developed in the 1950s and 1960s by the scientist Frank Rosenblatt, inspired by earlier work by Warren McCulloch and Walter Pitts. A perceptron takes several binary inputs, $x_{1},x_{2},…$, and produces a single binary output\n",
    "\n",
    "Sigmoid neurons are similar to perceptrons, but modified so that small changes in their weights and bias cause only a small change in their output. That's the crucial fact which will allow a network of sigmoid neurons to learn. Just like a perceptron, the sigmoid neuron has inputs, $x_{1},x_{2},…$. But instead of being just 0 or 1, these inputs can also take on any values between 0 and 1. \n",
    "\n",
    "Also just like a perceptron, the sigmoid neuron has weights for each input, $w_{1},w_{2},…$, and an overall bias, $b$. But the output is not 0 or 1. Instead, it's $σ(w⋅x+b)$, where $σ$ is called the sigmoid function, and is defined by:\n",
    "\n",
    "$$σ(z) ≡ \\frac{1}{1+e^{−z}}$$\n",
    "\n",
    "Or more explicitly:\n",
    "\n",
    "$$σ(z) ≡ \\frac{1}{1 + \\exp(-\\sum_{j} w_{j} x_{j} - b)}$$\n",
    "\n",
    "In the limit the sigmoid function turns to a heaviside function, the neuron becomes a perceptron.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1d77fd82d68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sigmoid function\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from math import exp\n",
    "\n",
    "xs = np.linspace(-6., 6.)\n",
    "ys = [1./(1 + exp(-x)) for x in xs]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plt.plot(xs, ys)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The smoothness of $σ$ means that small changes $Δwj$ in the weights and $Δb$ in the bias will produce a small change $Δoutput$ in the output from the neuron. In fact, calculus tells us that $Δoutput$ is well approximated by\n",
    "$$Δoutput≈ \\sum_{j}\\frac{∂output}{∂w_{j}}Δw_{j} + \\frac{∂output}{∂b}Δb$$\n",
    "\n",
    "This means that $Δoutput$ is a linear function of the changes $Δw_{j}$ and $Δb$ in the weights and bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The architecture of neural networks\n",
    "\n",
    "The leftmost layer in this network is called the input layer, and the neurons within the layer are called input neurons. The rightmost or output layer contains the output neurons, or, as in this case, a single output neuron. The middle layer is called a hidden layer, since the neurons in this layer are neither inputs nor outputs.\n",
    "\n",
    "While the design of the input and output layers of a neural network is often straightforward, there can be quite an art to the design of the hidden layers.\n",
    "\n",
    "<b>feedforward neural networks</b> - Networks where the output from one layer is used as input to the next layer.\n",
    "\n",
    "However, there are other models of artificial neural networks in which feedback loops are possible. These models are called <b>recurrent neural networks</b>. The idea in these models is to have neurons which fire for some limited duration of time, before becoming quiescent.\n",
    "\n",
    "Recurrent neural nets have been less influential than feedforward networks, in part because the learning algorithms for recurrent nets are (at least to date) less powerful. \n",
    "\n",
    "## A simple network to classify handwritten digits\n",
    "\n",
    "Three-layer neural network:\n",
    "\n",
    "![Image](Images/tikz12.PNG)\n",
    "\n",
    "- Training data for the network will consist of many <b>28 by 28</b> pixel images of scanned handwritten digits, and so the input layer contains 784 = 28×28 neurons.\n",
    "\n",
    "- The input pixels are greyscale, with a value of 0.0 representing white, a value of 1.0 representing black, and in between values representing gradually darkening shades of grey.\n",
    "\n",
    "- The second layer of the network is a hidden layer. We denote the number of neurons in this hidden layer by n, and we'll experiment with different values for n. The example shown illustrates a small hidden layer, containing just n=15 neurons.\n",
    "\n",
    "### MNIST dataset\n",
    "\n",
    "- train-images-idx3-ubyte: training set images \n",
    "- train-labels-idx1-ubyte: training set labels \n",
    "- t10k-images-idx3-ubyte:  test set images \n",
    "- t10k-labels-idx1-ubyte:  test set labels\n",
    "\n",
    "The training set contains 60000 examples, and the test set 10000 examples.\n",
    "\n",
    "The first 5000 examples of the test set are taken from the original NIST training set. The last 5000 are taken from the original NIST test set. The first 5000 are cleaner and easier than the last 5000.\n",
    "\n",
    "## Neural network design\n",
    "\n",
    "We'll use the notation x to denote a training input. It'll be convenient to regard each training input $x$ as a 28×28=784-dimensional vector. \n",
    "\n",
    "Define the quadratic cost function (mean squared error):\n",
    "\n",
    "$$C(w,b) = \\frac{1}{2n}\\sum_{x}\\left|\\left|y(x) - a\\right|\\right|^{2}$$\n",
    "\n",
    "Where $n$ is the total number of training inputs, $a$ is the vector of outputs from the network when $x$ is input, $w$ and $b$ denote the vector of weights and biases respectively.\n",
    "\n",
    "The choice of a quadratic cost function makes it easier to disentangle and optimise improvements from changes in the weights and biases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent\n",
    "\n",
    "The gradient vector of the cost function with variables $v$ is given by:\n",
    "\n",
    "$$∇C≡\\left(\\frac{∂C}{∂v_{1}},...,\\frac{∂C}{∂v_{m}}\\right)^{T}$$\n",
    "\n",
    "Choose $Δv$ so as to make $ΔC$ negative. In particular, suppose we choose:\n",
    "\n",
    "$$Δv=−η∇C$$\n",
    "\n",
    "where $η$ is a small, positive parameter (known as the learning rate). This gives us a way of following the gradient to a minimum, even when C is a function of many variables, by repeatedly applying the update rule:\n",
    "\n",
    "$$v→v′=v−η∇C$$\n",
    "\n",
    "Restating the gradient descent update rule, with the weights and biases replacing the variables $v_{j}$ we can write:\n",
    "\n",
    "$$w_{k}→w^{'}_{k}=w_{k} − η\\frac{∂C}{∂w_{k}}$$\n",
    "$$b_{l}→b^{'}_{l}=b_{l} − η\\frac{∂C}{∂b_{l}}$$\n",
    "\n",
    "\n",
    "### Stochastic gradient descent\n",
    "Computing the gradient for large numbers of inputs can be time consuming. An idea called <b>stochastic gradient descent</b> can be used to speed up learning. The idea is to estimate the gradient $∇C$ by computing $∇C_{x}$ for a small sample of randomly chosen training inputs. By averaging over this small sample it turns out that we can quickly get a good estimate of the true gradient $∇C$, and this helps speed up gradient descent, and thus learning. Stochastic gradient descent works by randomly picking out a small number $m$ of randomly chosen training inputs. We'll label those random training inputs $X_{1},X_{2},…,X_{m}$, and refer to them as a mini-batch.\n",
    "\n",
    "Stochastic gradient descent works by picking out a randomly chosen mini-batch of training inputs, and training with those,\n",
    "\n",
    "$$w_{k}→w^{'}_{k}=w_{k} − \\frac{η}{m}\\sum_{j}\\frac{∂C_{X_{j}}}{∂w_{k}}$$\n",
    "$$b_{l}→b^{'}_{l}=b_{l} − \\frac{η}{m}\\sum_{j}\\frac{∂C_{X_{j}}}{∂b_{l}}$$\n",
    "\n",
    "where the sums are over all the training examples $X_{j}$ in the current mini-batch. Then we pick out another randomly chosen mini-batch and train with those. And so on, until we've exhausted the training inputs, which is said to complete an <b>epoch</b> of training. At that point we start over with a new training epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network code (network.py)\n",
    "\n",
    "Constructor is array with the size defining the number of layers and the values representing the number of neurons per layer, for example a 2-3-1 network is defined:\n",
    "\n",
    "```python\n",
    "net = Network([2, 3, 1])\n",
    "```\n",
    "\n",
    "The biases and weights in the Network object are all initialized randomly, using a Gaussian distribution with mean 0 and standard deviation 1. In this example, the weights array is a matrix such that $w_{jk}$ is the weight for the connection between the k$^{th}$ neuron in the second layer, and the j$^{th}$ neuron in the third layer. The vector of activations of the third layer of neurons is given by:\n",
    "$$a′=σ(wa+b)$$\n",
    "\n",
    "The is used in the <i>feedforward</i> method.\n",
    "\n",
    "Learning is done via a stochastic gradient descent algorithm, defined in the method <i>SGD</i>, which takes as inputs: the training data array, number of epochs, mini-batch size, learning rate $\\eta$ and test data (optional).\n",
    "\n",
    "- The training_data is a list of tuples $(x, y)$ representing the training inputs and corresponding desired outputs.\n",
    "- If the optional argument test_data is supplied, then the program will evaluate the network after each epoch of training, and print out partial progress. This is useful for tracking progress, but slows things down substantially.\n",
    "\n",
    "The code works as follows:\n",
    "\n",
    "- In each epoch, it starts by randomly shuffling the training data, and then partitions it into mini-batches of the appropriate size. \n",
    "- Then for each mini_batch we apply a single step of gradient descent. This is done by the code <i>self.update_mini_batch(mini_batch, eta)</i>, which updates the network weights and biases according to a single iteration of gradient descent.\n",
    "\n",
    "The <i>update_mini_batch</i> method makes use of the <i>backpropagation</i> method which determines the gradients of the cost functions.\n",
    "\n",
    "### Output\n",
    "\n",
    "net.weights[0] = (30, 784)\n",
    "net.weights[1] = (10, 30)\n",
    "\n",
    "net.bias[0] = (30, 1)\n",
    "net.bias[1] = (10, 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2 - Backpropagation\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the heart of backpropagation is an expression for the partial derivative $∂C/∂w$ of the cost function $C$ with respect to any weight $w$ (or bias $b$) in the network. The expression tells us how quickly the cost changes when we change the weights and biases.\n",
    "\n",
    "In the following, $w^{l}_{jk}$ denotes the weight for the connection from the $k^{th}$ neuron in the $(l−1)^{th}$ layer to the $j^{th}$ neuron in the  $l^{th}$ layer. Similarly we use $b^{l}_{j}$ for the bias of the $j^{th}$ neuron in the $l^{th}$ layer. And we use $a^{l}_{j}$ for the activation of the $j^{th}$ neuron in the $l^{th}$ layer. \n",
    "\n",
    "![image](images/CH2_1.PNG)\n",
    "\n",
    "The activation $a^{l}_{j}$ of the $j^{th}$ neuron in the $l^{th}$ layer is related to the activations in the $(l−1)^{th}$ layer by the equation:\n",
    "\n",
    "$$a^{l}_{j} = σ\\left(\\sum_{k} w^{l}_{jk} a^{l-1}_{k} + b^{l}_{j}\\right)$$\n",
    "\n",
    "Vectorisation notation:\n",
    "\n",
    "$$σ(v) = σ(v_{j})$$\n",
    "\n",
    "Enables a simplification to the following:\n",
    "\n",
    "$$a^{l} = σ\\left(w^{l} a^{l-1} + b^{l}\\right)$$\n",
    "\n",
    "Define the weighted input to neurons in layer $l$: $z^{l} = w^{l} a^{l-1} + b^{l}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost function and backpropagation\n",
    "\n",
    "Two assumptions:\n",
    "- The cost function can be written as an average $C= \\frac{1}{n} \\sum_{x}C_{x}$ over cost functions $C_{x}$.\n",
    "- The cost is that it can be written as a function of the outputs from the neural network.\n",
    "\n",
    "Hadamard product: $s⊙t$ is a elementwise multiplication of vectors defined:\n",
    "\n",
    "$$(s⊙t)_{j} = s_{j}t_{j}$$\n",
    "\n",
    "\n",
    "Error $δ_{lj}$ of neuron $j$ in layer $l$ is given by:\n",
    "\n",
    "$$δ^{l}_{j} ≡ \\frac{∂C}{∂z^{l}_{j}}$$\n",
    "\n",
    "<b>The error in the output layer, $δ^{L}$</b>:\n",
    "\n",
    "$$δ^{L}_{j} =  \\frac{∂C}{∂a^{L}_{j}}σ′(z^{L}_{j})$$\n",
    "\n",
    "Which can be expressed more succinctly as:\n",
    "\n",
    "$$δ^{L} = ∇_{a}C⊙σ′(z^{L})$$\n",
    "\n",
    "<b>Equation for the error δl in terms of the error in the next layer, δl+1</b>:\n",
    "\n",
    "$$δ^{l}=((w^{l+1})^{T}δ^{l+1})⊙σ′(z^{l})$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
