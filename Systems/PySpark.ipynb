{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"float:left;font-size:20px;\">\n",
    "    <h1>PySpark</h1>\n",
    "</div><div style=\"float:right;\"><img src=\"../assets/banner.jpg\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import HiveContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row\n",
    "from pyspark import sql\n",
    "\n",
    "from pyspark import SparkConf, SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf()#.setAppName('Test App').setMaster('Local')\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "sqlContext = sql.SQLContext(sc)\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Spark DataFrame from an RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sc.parallelize([(1, 'Cat', 4, 10),\n",
    "                       (2, 'Dog', 4, 9),\n",
    "                       (3, 'Parrot', 2, 3)])\n",
    "\n",
    "data_df = spark.createDataFrame(data, ['ID', 'Animal', 'Legs', 'Popularity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----+----------+\n",
      "| ID|Animal|Legs|Popularity|\n",
      "+---+------+----+----------+\n",
      "|  1|   Cat|   4|        10|\n",
      "|  2|   Dog|   4|         9|\n",
      "|  3|Parrot|   2|         3|\n",
      "+---+------+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(ID=1, Animal='Cat', Legs=4, Popularity=10)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DataFrames are built of Row objects\n",
    "data_df.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Spark DataFrame from JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#json_df = spark.read.json('test.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a CSV table to a Spark DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the following in a `pyspark` console:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_csv = spark.read.csv(\"file:///home/tau/iris.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------+----------------+-----------------+----------------+------+\n",
      "|_c0|sepal length (cm)|sepal width (cm)|petal length (cm)|petal width (cm)|target|\n",
      "+---+-----------------+----------------+-----------------+----------------+------+\n",
      "|  0|              5.1|             3.5|              1.4|             0.2|   0.0|\n",
      "|  1|              4.9|             3.0|              1.4|             0.2|   0.0|\n",
      "|  2|              4.7|             3.2|              1.3|             0.2|   0.0|\n",
      "|  3|              4.6|             3.1|              1.5|             0.2|   0.0|\n",
      "|  4|              5.0|             3.6|              1.4|             0.2|   0.0|\n",
      "+---+-----------------+----------------+-----------------+----------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# View the contents:\n",
    "data_csv.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- sepal length (cm): double (nullable = true)\n",
      " |-- sepal width (cm): double (nullable = true)\n",
      " |-- petal length (cm): double (nullable = true)\n",
      " |-- petal width (cm): double (nullable = true)\n",
      " |-- target: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the schema\n",
    "data_csv.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------+----------------+-----------------+----------------+------+-------------------+\n",
      "|_c0|sepal length (cm)|sepal width (cm)|petal length (cm)|petal width (cm)|target|   petal area (cm2)|\n",
      "+---+-----------------+----------------+-----------------+----------------+------+-------------------+\n",
      "|  0|              5.1|             3.5|              1.4|             0.2|   0.0|0.27999999999999997|\n",
      "|  1|              4.9|             3.0|              1.4|             0.2|   0.0|0.27999999999999997|\n",
      "|  2|              4.7|             3.2|              1.3|             0.2|   0.0|               0.26|\n",
      "|  3|              4.6|             3.1|              1.5|             0.2|   0.0|0.30000000000000004|\n",
      "|  4|              5.0|             3.6|              1.4|             0.2|   0.0|0.27999999999999997|\n",
      "+---+-----------------+----------------+-----------------+----------------+------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# WithColumn returns a new dataframe with the added column\n",
    "data_df2 = data_csv.withColumn(\"petal area (cm2)\", data_csv[\"petal length (cm)\"] * data_csv[\"petal width (cm)\"])\n",
    "data_df2.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Monte Carlo to compute Pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pi ~ 3.1704\n"
     ]
    }
   ],
   "source": [
    "from random import random\n",
    "from operator import add\n",
    "def throw_darts(_):\n",
    "    x = random() * 2 - 1\n",
    "    y = random() * 2 - 1\n",
    "    return 1 if x ** 2 + y ** 2 <= 1 else 0\n",
    "\n",
    "n = 10000\n",
    "count = sc.parallelize(range(1, n + 1), 1).map(throw_darts).reduce(add)\n",
    "\n",
    "print(f'Pi ~ {4* count/n}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read book and perform map-reduce operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the execution plan in: http://localhost:4040/stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = sc.textFile(\"file:///home/tau/Downloads/2554-0.txt\")\\\n",
    "            .flatMap(lambda line: line.split(\" \"))\\\n",
    "            .map(lambda word: (word, 1))\\\n",
    "            .reduceByKey(lambda a, b: a + b)\\\n",
    "\n",
    "counts.saveAsTextFile(\"hdfs:///user/tau/2554-0_wordCount.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter an RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41.6 ms ± 2.12 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "arr = range(10000)\n",
    "rdd = sc.parallelize(arr, 4)\n",
    "odds = rdd.filter(lambda x: x % 2 != 0)\n",
    "odds.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using SciPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+\n",
      "| id|               val|\n",
      "+---+------------------+\n",
      "|  0|0.9453879644531373|\n",
      "|  1|0.7138517107833131|\n",
      "|  2|0.6439455957156267|\n",
      "+---+------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as f\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "# Create a DataFrame with the column 'val' filled with 1e6 random numbers\n",
    "big_df = (spark.range(0, 1000000)\n",
    "               .withColumn('val', f.rand()))\n",
    "\n",
    "big_df.cache()\n",
    "big_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tau/anaconda3/lib/python3.7/site-packages/pyspark/sql/pandas/functions.py:386: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  \"in the future releases. See SPARK-28264 for more details.\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+-------------------+\n",
      "| id|               val|        probability|\n",
      "+---+------------------+-------------------+\n",
      "|  0|0.9453879644531373|0.25517192766594166|\n",
      "|  1|0.7138517107833131|0.30921122310314325|\n",
      "|  2|0.6439455957156267|  0.324239934612677|\n",
      "|  3|0.7710043804297324|0.29636531582975634|\n",
      "|  4|0.6429033620151852| 0.3244574424054605|\n",
      "+---+------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Return the gaussian PDF - Python UDF (User defined function)\n",
    "@f.pandas_udf('double', f.PandasUDFType.SCALAR)\n",
    "def pandas_pdf(v):\n",
    "    return pd.Series(stats.norm.pdf(v))\n",
    "\n",
    "big_df.withColumn('probability', pandas_pdf(big_df.val)\n",
    "                 ).show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.7 ms ± 14.5 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# Slow method\n",
    "def test_pandas_pdf():\n",
    "    return (big_df\n",
    "           .withColumn('probability', pandas_pdf(big_df.val))\n",
    "           .agg(f.count(f.col('probability')))\n",
    "           #.show()\n",
    "           )\n",
    "\n",
    "%timeit -n 1 test_pandas_pdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.8 ms ± 4.26 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# Fast method - Vectorised UDF (User defined function)\n",
    "@f.udf('double')\n",
    "def pdf(v):\n",
    "    return float(stats.norm.pdf.pdf(v))\n",
    "\n",
    "def test_pdf():\n",
    "    return (big_df\n",
    "           .withColumn('probability', pdf(big_df.val))\n",
    "           .agg(f.count(f.col('probability')))\n",
    "           #.show()\n",
    "           )\n",
    "\n",
    "%timeit -n 1 test_pdf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporary tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|Animal|\n",
      "+------+\n",
      "|   Cat|\n",
      "|   Dog|\n",
      "|Parrot|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_df.createTempView('data_df_view')  # Create a new temporary table with specified name\n",
    "spark.sql('''SELECT Animal FROM data_df_view''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+\n",
      "|Legs|AnimalCount|\n",
      "+----+-----------+\n",
      "|   2|          1|\n",
      "|   4|          2|\n",
      "+----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_df.createOrReplaceTempView('data_df_view')  # Create or replace a new temporary table with specified name\n",
    "spark.sql('''SELECT Legs, COUNT(*) AS AnimalCount FROM data_df_view\n",
    "             GROUP BY Legs''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+\n",
      "|Animal|Legs|\n",
      "+------+----+\n",
      "|   Cat|   4|\n",
      "|   Dog|   4|\n",
      "|Parrot|   2|\n",
      "+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_df.select('Animal', 'Legs').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----+----------+\n",
      "| ID|Animal|Legs|Popularity|\n",
      "+---+------+----+----------+\n",
      "|  1|   Cat|   4|        10|\n",
      "|  2|   Dog|   4|         9|\n",
      "+---+------+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_df.filter(data_df.Legs == 4).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|Legs|count|\n",
      "+----+-----+\n",
      "|   2|    1|\n",
      "|   4|    2|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_df.groupBy('Legs').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = data_df.toPandas()\n",
    "type(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To Install HIVE:\n",
    "- Download HIVE: http://archive.apache.org/dist/hive/hive-3.1.2/\n",
    "- Add to .bashrc:\n",
    "    ```bash\n",
    "    export HIVE_HOME=/home/tau/apache-hive-3.1.2-bin\n",
    "    export PATH=$PATH:$HIVE_HOME/bin\n",
    "    ```\n",
    "- This may require copying a compatible jar file: https://issues.apache.org/jira/browse/HIVE-22915\n",
    "\n",
    "- This requires you have mysql installed:\n",
    "```bash\n",
    "sudo apt install mysql-server\n",
    "sudo mysql_secure_installation\n",
    "```\n",
    "- And a `hive-site.xml` file in the conf directory (https://stackoverflow.com/questions/35449274/java-lang-runtimeexception-unable-to-instantiate-org-apache-hadoop-hive-ql-meta)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save in a Hive table in ORC dataformat\n",
    "from pyspark.sql import HiveContext\n",
    "hc = HiveContext(sc)\n",
    "df_csv.write.format(\"orc\").saveAsTable(\"employees\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
